{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS Enabled\n",
      "Finished reading documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# !pip uninstall torch -y\n",
    "# !pip install torch torchvision torchaudio --no-cache-dir --index-url https://download.pytorch.org/whl/cpu\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hardware Acceleration\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print('MPS Enabled')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('CUDA Enabled')\n",
    "\n",
    "# Load the datasets\n",
    "a = pd.read_csv('/Users/brianhakam/Desktop/ncVEP/chr_21_observed_use.csv')\n",
    "b = pd.read_csv('/Users/brianhakam/Desktop/ncVEP/chr_21_synthetic_2_PUL_pred.csv')\n",
    "a = a.drop(['chr', 'start', 'end', 'annot', 'Substitution'], axis=1)\n",
    "dfObs = a.copy()\n",
    "dfSyn = b.copy()\n",
    "\n",
    "print('Finished reading documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading documents\n"
     ]
    }
   ],
   "source": [
    "#Process Data\n",
    "# Ensure both datasets have the same number of samples\n",
    "dfObs = a.copy()\n",
    "dfSyn = b.copy()\n",
    "\n",
    "print('Finished reading documents')\n",
    "dfObs = dfObs.sample(n=len(dfSyn), random_state=42)\n",
    "\n",
    "# dfObs = dfObs.drop(['chr', 'start', 'end', 'annot', 'Substitution'], axis=1)\n",
    "\n",
    "# Prepare the data\n",
    "dfObs['label'] = 0\n",
    "dfSyn['label'] = 1\n",
    "# dfObs['isReal'] = int(1)\n",
    "# dfSyn['isReal'] = int(0)\n",
    "df = pd.concat([dfObs, dfSyn], ignore_index=True)\n",
    "\n",
    "df = df.replace('a', 'A')\n",
    "df = df.replace('c', 'C')\n",
    "df = df.replace('g', 'G')\n",
    "df = df.replace('t', 'T')\n",
    "# df = df.replace('A', 'a')\n",
    "# df = df.replace('C', 'c')\n",
    "# df = df.replace('G', 'g')\n",
    "# df = df.replace('T', 't')\n",
    "\n",
    "# Identify non-numeric columns (excluding 'label')\n",
    "non_numeric_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "non_numeric_columns = ['+1', '-1', 'Ref', 'Alt']\n",
    "\n",
    "for column in non_numeric_columns:\n",
    "    df[column] = df[column].astype('category').cat.codes\n",
    "\n",
    "#Put names of columns in df as a list\n",
    "featureList = df.columns\n",
    "# print(non_numeric_columns)\n",
    "# if 'label' in non_numeric_columns:\n",
    "#     non_numeric_columns.remove('label')  # Ensure 'label' is not dropped from y\n",
    "\n",
    "# Option 1: Drop non-numeric columns\n",
    "y = df['label'].values\n",
    "X = df.drop(['label'], axis=1).values\n",
    "\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoader for mini-batch gradient descent\n",
    "batch_size = 3200\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                              torch.tensor(y_train, dtype=torch.float32).view(-1, 1))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Neural Network Model\n",
    "# # Define the model\n",
    "# class SimpleNN(nn.Module):\n",
    "#     def __init__(self, input_dim, *layers):\n",
    "#         super(SimpleNN, self).__init__()\n",
    "        \n",
    "#         # Create a list of layer sizes, starting with input_dim\n",
    "#         layer_sizes = [input_dim] + list(layers)\n",
    "        \n",
    "#         self.layers = nn.ModuleList()\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "#             self.layers.append(nn.ReLU())\n",
    "        \n",
    "#         self.layers.append(nn.Linear(layer_sizes[-1], 1))\n",
    "#         self.layers.append(nn.Sigmoid())\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         return x\n",
    "\n",
    "# # Initialize the model\n",
    "# input_dim = X_train.shape[1]\n",
    "# model_layers = (100, 100, 100, 100, 100, 100, 50, 32, 16, 8, 4)\n",
    "# model = SimpleNN(input_dim, *model_layers).to(device)\n",
    "\n",
    "# # Training setup\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# # Training the model with mini-batch gradient descent\n",
    "# num_epochs = 2\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(X_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     # Print loss for each epoch\n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "# # Testing the model with accuracy calculation\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_test_tensor)\n",
    "#     predicted = (outputs > 0.5).float()  # Threshold at 0.5 for binary classification\n",
    "#     correct = (predicted == y_test_tensor).sum().item()\n",
    "#     accuracy = correct / y_test_tensor.size(0)\n",
    "#     print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "# !pip install xgboost\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define and train the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=1,\n",
    "    n_estimators=19,\n",
    "    epochs=1,\n",
    "    include_categorical=True,\n",
    ")\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(\n",
    "#     max_depth=5,\n",
    "#     learning_rate=.1,\n",
    "#     n_estimators=30,\n",
    "#     # use_label_encoder=False,\n",
    "#     eval_metric='logloss'  # Use log loss for binary classification\n",
    "# )\n",
    "\n",
    "# params = {\n",
    "#     'objective': 'binary:logistic',\n",
    "#     #eval metric is fraction of misclassified positives\n",
    "#     'eval_metric': 'error',\n",
    "#     'tree_method': 'hist',\n",
    "#     'enable_categorical': True\n",
    "# }\n",
    "\n",
    "# # Initialize the model\n",
    "# xgb_model = xgb.XGBClassifier(\n",
    "#     n_estimators=10000,\n",
    "#     **params,\n",
    "# )\n",
    "\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate accuracy\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "# Add prediction scores to the dataframe\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "# Calculate accuracy for observed data\n",
    "y_pred_obs = xgb_model.predict(X_train)\n",
    "# Identify wrong predictions\n",
    "wrong_predictions = X_test[y_test != y_pred]\n",
    "\n",
    "df['prediction_score'] = xgb_model.predict_proba(X)[:, 1]\n",
    "\n",
    "\n",
    "# Print 10 examples of data predicted wrongly\n",
    "# print(wrong_predictions[:10])\n",
    "\n",
    "\n",
    "# # Calculate weighted accuracy\n",
    "# weighted_correct = np.sum((y_pred == y_test) * (1 - np.abs(y_pred - y_test)))\n",
    "# weighted_accuracy = weighted_correct / len(y_test)\n",
    "# print(f'Weighted Accuracy: {weighted_accuracy:.4f}')\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(xgb_model, importance_type='weight', max_num_features=100)\n",
    "ax = plt.gca()\n",
    "ax.set_yticklabels([featureList[int(label.get_text()[1:])] for label in ax.get_yticklabels()])\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# Create a new dataset to predict if a data point would be accurately predicted\n",
    "accuracy_labels = (y_test == y_pred).astype(int)\n",
    "\n",
    "# Split the new dataset into training and test sets\n",
    "X_train_acc, X_test_acc, y_train_acc, y_test_acc = train_test_split(\n",
    "    X_test, accuracy_labels, train_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "# Define and train a new XGBoost model for accuracy prediction\n",
    "xgb_model_acc = xgb.XGBClassifier(\n",
    "    max_depth=2,\n",
    "    learning_rate=1,\n",
    "    n_estimators=300,\n",
    "    eval_metric='error'\n",
    ")\n",
    "\n",
    "xgb_model_acc.fit(X_train_acc, y_train_acc)\n",
    "\n",
    "# Make predictions and evaluate accuracy\n",
    "y_pred_acc = xgb_model_acc.predict(X_test_acc)\n",
    "accuracy_acc = accuracy_score(y_test_acc, y_pred_acc)\n",
    "print(f'Accuracy of accuracy prediction model: {accuracy_acc:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dfObs.columns)\n",
    "# # print(dfObs.to_numpy()[0])\n",
    "# # print(dfSyn.columns)\n",
    "# # print(dfSyn.to_numpy()[0])\n",
    "\n",
    "# # print(len(dfObs.to_numpy()[0]))\n",
    "# # print(len(dfSyn.to_numpy()[0]))\n",
    "# for col in range(len(wrong_predictions[0])):\n",
    "#         print(f\"{dfObs.columns[col]}: {wrong_predictions[ti][col]}\")\n",
    "# for ti in range(10):\n",
    "#     # print(wrong_predictions[ti])\n",
    "#     for col in range(len(wrong_predictions[ti])):\n",
    "#         print(f\"{dfObs.columns[col]}: {wrong_predictions[ti][col]}\")\n",
    "#     print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
